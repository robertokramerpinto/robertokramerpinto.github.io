# Azure DP-100 Certification

Exam DP-100: Designing and Implementing a Data Science Solution on Azure

Topics: 

> Set up an Azure Machine Learning Workspace (30-35%)
*  Create an Azure Machine Learning Workspace
    * Create an azure ml workspace
    * Configure workspace settings
    * Manage a workspace by using Azure ML Studio
* Managing Data Objects in Azure ML workspace
    * Register and maintain data storages
    * Create and manage datasets
* Manage experiment compute contexts
    * Create a compute instance
    * Determine appropriate compute specifications for a training workload
    * Create compute targets for experiments and trainings

> Run Experiments and Train Models (30-35%)
* Create models by using Azure Machine Learning Designer
    * create a training pipeline by using Azure Machine Learning designer
    * ingest data in a designer pipeline
    * use designer modules to define a pipeline data flow
    * use custom code modules in designer
* Run training scripts in an Azure Machine Learning workspace
    * create and run an experiment by using the Azure Machine Learning SDK
    * consume data from a data store in an experiment by using the Azure Machine Learning SDK
    * consume data from a dataset in an experiment by using the Azure Machine Learning SDK
    * choose an estimator for a training experiment
* Generate metrics from an experiment run
    * log metrics from an experiment run
    * retrieve and view experiment outputs
    * use logs to troubleshoot experiment run errors
* Automate the model training process
    * create a pipeline by using the SDK
    * pass data between steps in a pipeline
    * run a pipeline
    * monitor pipeline runs

> Optimize and Manage Models (20-25%)
* Use Automated ML to create optimal models
    * use the Automated ML interface in Azure Machine Learning studio
    * use Automated ML from the Azure Machine Learning SDK
    * select scaling functions and pre-processing options
    * determine algorithms to be searched
    * define a primary metric
    * get data for an Automated ML run
    * retrieve the best model
* Use Hyperdrive to tune hyperparameters
    * select a sampling method
    * define the search space
    * define the primary metric
    * define early termination options
    * find the model that has optimal hyperparameter values
* Use model explainers to interpret models
    * select a model interpreter
    * generate feature importance data
* Manage models
    * register a trained model
    * monitor model history
    * monitor data drift
    
> Deploy and Consume Models (20-25%)
* Create production compute targets
    * consider security for deployed services
    * evaluate compute options for deployment
* Deploy a model as a service
    * configure deployment settings
    * consume a deployed service
    * troubleshoot deployment container issues
* Create a pipeline for batch inferencing
    * publish a batch inferencing pipeline
    * run a batch inferencing pipeline and obtain outputs
* Publish a designer pipeline as a web service
    * create a target compute resource
    * configure an Inference pipeline
    * consume a deployed endpoint

# Introduction to Azure Machine Learning

## Data Science Lifecycle

![](/assets/azure/cert/dp100/1.png)

There are 5 major stages in the Data Science Lifecycle: 

> Business Understanding
- Problem to solve
- Identify data sources
- Determine accuracy

> Data Acquisition
- Data Ingestion (Azure Blob, SQL DB, Hive Tables)
- Data Ingestion Pipeline (Using tools like DataFactory, Databricks, Airflow) 
- Data Wrangling
- Data Exploration

> Modeling
- Feature Engineering
- Model Training 
- Model Evaluation

> Deployment 
- Expose model with open API interface
- Dump data into DBs

> Customer Acceptance
- Presenting ML results to end user 
- System validation
- Project hand-off --> move it into production

### Business Understanding

#### Defining Objectives

Defining objectives: Business understanding & SMART metrics

Here the main goal is to define business problems/solutions using advanced analytics. We here need to start identifying
what data we have/need in order to solve the problem. 

To define the problem properly, we need to start asking questions. We can do that through the SMART Metrics approach.

**SMART Goal**

SMART relates to the process of creating Specific, Measurable, Achievable, Relevant and Time-Related objectives.

- Specific: 5 Ws (Who, what, why, where, which)
- Measurable: Be able to track progress
- Achievable: Make your goals ambitious, but achievable
- Relevant: Ensure that your goal is relevant to the company's vision & creates value
- Time-Related: Use deadlines to track progress

**Identify Data Sources**
- Select only data that's relevant to the objectives (scope) of the project
- Is the data an accurate measure of the model target?

**Artifacts**

We can also use artifacts to help the project development at this initial stage. Artifacts are documents outputs at each 
step of the project. 
 
* Charter document
    * Early level documentation of the project (risks, costs, major goals, milestones)
* Data sources
    * Location and data to be used in the problem
    * Data dictionaries (describe data in data sources, schemas, entity relation documents) 

## Azure ML 

Azure machine learning is an fully equiped cloud environment designed to deliver end-to-end ML solutions. It integrates 
several Azure services, from data, modeling, DevOps and other tools, facilitating a robust implementation of complex
analytical solutions. 

![](/assets/azure/cert/dp100/2.png)

**Azure ML Workspace**

- The Azure ML Workspace is the top level resource for Azure ML solutions.
- Centralized Environment to use ML Tools and objects

A workspace is used to experiment, train, and deploy machine learning models. It's a centralized place to work with all 
the artifacts you create when you use Azure Machine Learning. 
The workspace keeps a history of all training runs, including logs, metrics, output, and a snapshot of your scripts. 
You use this information to determine which training run produces the best model.

- Each workspace is tied to an Azure subscription and resource group, and has an associated SKU.

### Major Components of the Azure Machine Learning Workspace

![](/assets/azure/cert/dp100/3.png)

> Compute Instances
- Cloud resources with Python environment

> User Roles
- Share a workspace with other users, teams or projects

> Compute Targets
- Used to run experiments

> Experiments
- Train runs to build models

> Pipelines
- Reusable workflows for training and retraining models

> Datasets
- Aid with data management

> Registered Model 
- Created after we are ready to deploy our models

> Deployment Endpoint
- Final step, a combination of the registered model and scoring script

## Compute

![](/assets/azure/cert/dp100/5.png)

Compute resources will provide us the computation power we need to execute our ML workflow. They are divided into 2 major
groups: compute instances and clusters. 

### Compute Instance 

We can choose from a selection of CPU or GPU instances preconfigured with popular tools such as JupyterLab, Jupyter, 
and RStudio, ML packages, deep learning frameworks, and GPU drivers to process our ML workflows.

![](/assets/azure/cert/dp100/6.png)

When creating a compute instance we can choose CPU/GPU computation, memory and disk sizes, as well as advanced
network configuration. 

## Notebooks

We can work with jupyter notebooks through the Azure ML workspace. 
![](/assets/azure/cert/dp100/4.png)

In order to execute those notebooks, we need to have compute instances available and attached to those notebooks. 

![](/assets/azure/cert/dp100/7.png)

Notebooks in Azure exist in 2 environments: inside the ML Studio and in a separate environmnet Azure Notebooks. 
Azure notebooks is more complete but it's easier to launch notebooks within ML Studio. 

**Jupyter x Zeppelin**

Zeppelin Notebooks is another different option for Notebooks usage. Zeppelin is more favored to Apache frameworks.
Whenever working with Apache open-source tools and Java, Zeppelin notebooks are more indicated by the community.

### Notebook Examples

#### Connect to a Workspace

First let's connect to an existing ML Workspace

**Using a config file**

Here in this example, we can download a config file through Azure portal (settings from workspace) and load its content:

````json
{
    "subscription_id": "974f9871-2375-47c7-bfd5-54e55b74fbdd",
    "resource_group": "cloudgurutraining",
    "workspace_name": "test_1"
}
````
````python
#!pip install azureml
#!pip install azureml-core
from azureml.core import Workspace

# Connecting to Existing Workspace
config_path = 'config.json' # download and save config file 

ws = Workspace.from_config()
````
This .from_config() method in this case will prompt a window so we can authentica the connection manually. 

**Automatic Authentication**

When setting up a machine learning workflow as an automated process, it's recommend 
using Service Principal Authentication. This approach decouples the authentication from any specific user login, and allows managed access control.

* Azure Portal >> Azure Active Directory >> Apps Registrations >> Create new application
* Copy Application (client) ID and Tentant (directory) ID 
* Select certificates & secrets (left menu) >> +new client secret >> copy client secret
* Go to IAM from the ML resource and add role to the registered application (add role assignment): grant access to resources for the specific tenant_id.  
* Recommendation is not to hardcode these passwords but instead use environment variables ($env:AZUREML_PASSWORD = "my-password")

````python
import os
from azureml.core import Workspace
from azureml.core.authentication import ServicePrincipalAuthentication

my_application_id = "e8f02d4d-9d0b-4abb-93ed-ed365ebee25f"
my_tenant_id = "e2c5fd58-d9bc-4c31-9495-bad58ae11f15"
secret = "_w_pL4RF5h.4FKysfN3.dlqtM~X-2aNtT1"

#svc_pr_password = os.environ.get("AZUREML_PASSWORD")

svc_pr = ServicePrincipalAuthentication(
    tenant_id=my_tenant_id,
    service_principal_id=my_application_id,
    service_principal_password=secret)

ws = Workspace(
    subscription_id="974f9871-2375-47c7-bfd5-54e55b74fbdd",
    resource_group="cloudgurutraining",
    workspace_name="test_1",
    auth=svc_pr
    )

print("Found workspace {} at location {}".format(ws.name, ws.location))
>> Found workspace test_1 at location eastus
````
From the workspace object we can continue developing the ml pipeline directly. 







**Example: Creating a cluster to process data**

A general data processing workflow is to run up a cluster to process data and terminate the cluster after the process
is completed. Let's see how we can do this through Azure ML Notebook. 

> Steps
* Create/Start compute instance and attach it to the notebook
* 



