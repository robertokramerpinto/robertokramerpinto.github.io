# Azure DP-100 Certification

Exam DP-100: Designing and Implementing a Data Science Solution on Azure

Topics: 

> Set up an Azure Machine Learning Workspace (30-35%)
*  Create an Azure Machine Learning Workspace
    * Create an azure ml workspace
    * Configure workspace settings
    * Manage a workspace by using Azure ML Studio
* Managing Data Objects in Azure ML workspace
    * Register and maintain data storages
    * Create and manage datasets
* Manage experiment compute contexts
    * Create a compute instance
    * Determine appropriate compute specifications for a training workload
    * Create compute targets for experiments and trainings

> Run Experiments and Train Models (30-35%)
* Create models by using Azure Machine Learning Designer
    * create a training pipeline by using Azure Machine Learning designer
    * ingest data in a designer pipeline
    * use designer modules to define a pipeline data flow
    * use custom code modules in designer
* Run training scripts in an Azure Machine Learning workspace
    * create and run an experiment by using the Azure Machine Learning SDK
    * consume data from a data store in an experiment by using the Azure Machine Learning SDK
    * consume data from a dataset in an experiment by using the Azure Machine Learning SDK
    * choose an estimator for a training experiment
* Generate metrics from an experiment run
    * log metrics from an experiment run
    * retrieve and view experiment outputs
    * use logs to troubleshoot experiment run errors
* Automate the model training process
    * create a pipeline by using the SDK
    * pass data between steps in a pipeline
    * run a pipeline
    * monitor pipeline runs

> Optimize and Manage Models (20-25%)
* Use Automated ML to create optimal models
    * use the Automated ML interface in Azure Machine Learning studio
    * use Automated ML from the Azure Machine Learning SDK
    * select scaling functions and pre-processing options
    * determine algorithms to be searched
    * define a primary metric
    * get data for an Automated ML run
    * retrieve the best model
* Use Hyperdrive to tune hyperparameters
    * select a sampling method
    * define the search space
    * define the primary metric
    * define early termination options
    * find the model that has optimal hyperparameter values
* Use model explainers to interpret models
    * select a model interpreter
    * generate feature importance data
* Manage models
    * register a trained model
    * monitor model history
    * monitor data drift
    
> Deploy and Consume Models (20-25%)
* Create production compute targets
    * consider security for deployed services
    * evaluate compute options for deployment
* Deploy a model as a service
    * configure deployment settings
    * consume a deployed service
    * troubleshoot deployment container issues
* Create a pipeline for batch inferencing
    * publish a batch inferencing pipeline
    * run a batch inferencing pipeline and obtain outputs
* Publish a designer pipeline as a web service
    * create a target compute resource
    * configure an Inference pipeline
    * consume a deployed endpoint


# Introduction to Azure Machine Learning


## Azure ML 

Azure machine learning is an fully equiped cloud environment designed to deliver end-to-end ML solutions. It integrates 
several Azure services, from data, modeling, DevOps and other tools, facilitating a robust implementation of complex
analytical solutions. 

![](/assets/azure/cert/dp100/2.png)

**Why to use Azure Machine Learning?**
- Simplify ML building models with AutomatedML
- Easy scaling model training in a cloud environment
- Use Python SDK and any Python open source frameworks & tools
- Manage workflows with DevOps for machine learning
- Simple deployment with cloud hosted endpoints

User can interact with Azure ML resources in serveral ways. For example:
![](/assets/azure/cert/dp100/8.png)

This is also a general picture of the end-to-end Ml cycle integrating several Azure ML tools:
![](/assets/azure/cert/dp100/9.png)



**Azure ML Workspace**

- The Azure ML Workspace is the top level resource for Azure ML solutions.
- Centralized Environment to use ML Tools and objects

A workspace is used to experiment, train, and deploy machine learning models. It's a centralized place to work with all 
the artifacts you create when you use Azure Machine Learning. 
The workspace keeps a history of all training runs, including logs, metrics, output, and a snapshot of your scripts. 
You use this information to determine which training run produces the best model.

- Each workspace is tied to an Azure subscription and resource group, and has an associated SKU.

### Major Components of the Azure Machine Learning Workspace

![](/assets/azure/cert/dp100/3.png)

> Compute Instances
- Cloud resources with Python environment
- User can use compute instances to develop and run ML pipelines independently 

> User Roles
- Share a workspace with other users, teams or projects

> Compute Targets
- Used to run experiments
- From a compute instance we can attache ML pipelines to compute targets so our jobs can be executed

> Experiments
- Train runs to build models

> Pipelines
- Reusable workflows for training and retraining models

> Datasets
- Aid with data management

> Registered Model 
- Created after we are ready to deploy our models

> Notebooks 
- We can launch notebooks to run ML pipelines

> Automated ML
- AutoML framework to develop automated ml pipelines

> Deployment Endpoint
- Final step, a combination of the registered model and scoring script

## Compute

![](/assets/azure/cert/dp100/5.png)

Compute resources will provide us the computation power we need to execute our ML workflow. They are divided into 2 major
groups: compute instances and clusters. 

### Compute Instance 

We can choose from a selection of CPU or GPU instances preconfigured with popular tools such as JupyterLab, Jupyter, 
and RStudio, ML packages, deep learning frameworks, and GPU drivers to process our ML workflows.

![](/assets/azure/cert/dp100/6.png)

When creating a compute instance we can choose CPU/GPU computation, memory and disk sizes, as well as advanced
network configuration. 

## Notebooks

We can work with jupyter notebooks through the Azure ML workspace. 
![](/assets/azure/cert/dp100/4.png)

In order to execute those notebooks, we need to have compute instances available and attached to those notebooks. 

![](/assets/azure/cert/dp100/7.png)

Notebooks in Azure exist in 2 environments: inside the ML Studio and in a separate environmnet Azure Notebooks. 
Azure notebooks is more complete but it's easier to launch notebooks within ML Studio. 

**Jupyter x Zeppelin**

Zeppelin Notebooks is another different option for Notebooks usage. Zeppelin is more favored to Apache frameworks.
Whenever working with Apache open-source tools and Java, Zeppelin notebooks are more indicated by the community.

### Notebook Examples

#### Connect to a Workspace

First let's connect to an existing ML Workspace

**Using a config file**

Here in this example, we can download a config file through Azure portal (settings from workspace) and load its content:

````json
{
    "subscription_id": "974f9871-2375-47c7-bfd5-54e55b74fbdd",
    "resource_group": "cloudgurutraining",
    "workspace_name": "test_1"
}
````
````python
#!pip install azureml
#!pip install azureml-core
from azureml.core import Workspace

# Connecting to Existing Workspace
config_path = 'config.json' # download and save config file 

ws = Workspace.from_config()
````
This .from_config() method in this case will prompt a window so we can authentica the connection manually. 

**Automatic Authentication**

When setting up a machine learning workflow as an automated process, it's recommend 
using Service Principal Authentication. This approach decouples the authentication from any specific user login, and allows managed access control.

* Azure Portal >> Azure Active Directory >> Apps Registrations >> Create new application
* Copy Application (client) ID and Tentant (directory) ID 
* Select certificates & secrets (left menu) >> +new client secret >> copy client secret
* Go to IAM from the ML resource and add role to the registered application (add role assignment): grant access to resources for the specific tenant_id.  
* Recommendation is not to hardcode these passwords but instead use environment variables ($env:AZUREML_PASSWORD = "my-password")

````python
import os
from azureml.core import Workspace
from azureml.core.authentication import ServicePrincipalAuthentication

my_application_id = "e8f02d4d-9d0b-4abb-93ed-ed365ebee25f"
my_tenant_id = "e2c5fd58-d9bc-4c31-9495-bad58ae11f15"
secret = "_w_pL4RF5h.4FKysfN3.dlqtM~X-2aNtT1"

#svc_pr_password = os.environ.get("AZUREML_PASSWORD")

svc_pr = ServicePrincipalAuthentication(
    tenant_id=my_tenant_id,
    service_principal_id=my_application_id,
    service_principal_password=secret)

ws = Workspace(
    subscription_id="974f9871-2375-47c7-bfd5-54e55b74fbdd",
    resource_group="cloudgurutraining",
    workspace_name="test_1",
    auth=svc_pr
    )

print("Found workspace {} at location {}".format(ws.name, ws.location))
>> Found workspace test_1 at location eastus
````
From the workspace object we can continue developing the ml pipeline directly. 







**Example: Creating a cluster to process data**

A general data processing workflow is to run up a cluster to process data and terminate the cluster after the process
is completed. Let's see how we can do this through Azure ML Notebook. 

> Steps
* Create/Start compute instance and attach it to the notebook
* 



